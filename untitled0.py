# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QkKO22MZSH92MpovxnkZ3hnKh82CXO5L
"""

# Question 2
text = "Chris met Alex at Apple headquarters in California. He told him about the new iPhone launch."

import re

def warn_pronouns(s: str):
    if re.search(r"\b(he|she|they)\b", s, flags=re.I):
        print("Warning: Possible pronoun ambiguity detected!")

def try_spacy_ner(s: str):
    """Try spaCy NER if spaCy + model are already present."""
    try:
        import spacy
        try:
            nlp = spacy.load("en_core_web_sm")
        except Exception:
            # If the model isn't present, bail to fallback instead of trying to pip-install (keeps this cell reliable)
            return None
        doc = nlp(s)
        ents = [(ent.text, ent.label_) for ent in doc.ents]
        return ents
    except Exception:
        return None

def fallback_rule_based_ner(s: str):
    """
    Lightweight, zero-dependency NER heuristic:
    - Capitalized tokens â†’ PERSON/PROPN candidates
    - Gazetteers for ORG/GPE/PRODUCT
    - Simple multiword grouping (adjacent capitalized tokens)
    """
    ORGS = {"Apple", "Google", "Microsoft", "Amazon", "Meta", "OpenAI"}
    GPES = {"California", "Texas", "New York", "Florida", "USA", "United", "States"}
    PRODUCTS = {"iPhone", "MacBook", "Windows", "Android", "iPad"}

    # Basic tokenization preserving caps
    tokens = re.findall(r"[A-Za-z]+(?:'[A-Za-z]+)?", s)
    ents = []

    i = 0
    while i < len(tokens):
        tok = tokens[i]
        # ORG / GPE / PRODUCT direct matches
        if tok in ORGS:
            ents.append((tok, "ORG"))
            i += 1
            continue
        if tok in GPES:
            ents.append((tok, "GPE"))
            i += 1
            continue
        if tok in PRODUCTS:
            ents.append((tok, "PRODUCT"))
            i += 1
            continue

        # Proper-noun runs (PERSON/PROPN candidates): e.g., "Chris", "Alex"
        if tok[0].isupper():
            j = i + 1
            span = [tok]
            # group consecutive capitalized tokens (simple)
            while j < len(tokens) and tokens[j][0].isupper():
                span.append(tokens[j])
                j += 1
            candidate = " ".join(span)

            # Re-label if span contains known org/gpe words
            if any(w in ORGS for w in span):
                ents.append((candidate, "ORG"))
            elif any(w in GPES for w in span):
                ents.append((candidate, "GPE"))
            else:
                # Heuristic: treat single capitalized names as PERSON
                ents.append((candidate, "PERSON"))
            i = j
            continue

        i += 1

    # Deduplicate while preserving order
    seen = set()
    unique = []
    for e in ents:
        if e not in seen:
            seen.add(e)
            unique.append(e)
    return unique

# Run
warn_pronouns(text)
entities = try_spacy_ner(text)
if entities is None:
    # Fallback so the cell always works
    entities = fallback_rule_based_ner(text)

print("Entities:", entities)

# ------------------------------------------------------------
#Question 1
# Minimal, zero-dependency NLP pipeline
# 1) Tokenization  2) Stopword removal  3) Lemmatization  4) POS filter (NOUN/VERB/PROPN)
# ------------------------------------------------------------

import re

text = "John enjoys playing football while Mary loves reading books in the library."

# 1) Tokenize (simple regex for alphabetic tokens + apostrophes)
def tokenize(s):
    return re.findall(r"[A-Za-z]+(?:'[A-Za-z]+)?", s)

# 2) Stopwords (compact set; add/remove as needed)
STOPWORDS = {
    "a","an","the","and","or","but","if","while","in","on","at","to","for","of","with","without","into","from",
    "is","am","are","was","were","be","been","being","as","by","than","then","so","that","this","these","those"
}

# 3) Tiny rule-based POS tagger (heuristics for nouns/verbs/proper nouns)
#    - PROPN: capitalized tokens that aren't sentence-initial articles, etc.
#    - VERB: endings (-ing/-ed/-en/-es/-s) or in a small verb lexicon
#    - NOUN: otherwise (fallback) or known nouns
VERB_LEX = {"enjoy","love","read","play","have","do","go","make","say","get","see","know"}
NOUN_LEX = {"football","book","library","books"}  # can extend as needed

def guess_pos(tok):
    low = tok.lower()
    if tok[0].isupper() and low not in STOPWORDS:
        return "PROPN"  # Proper noun
    # Verb heuristics
    if (low in VERB_LEX or
        low.endswith(("ing","ed","en")) or
        (low.endswith("es") and len(low) > 3) or
        (low.endswith("s") and len(low) > 3 and not low.endswith("ss"))):
        return "VERB"
    # Known nouns or default to noun if alphabetic and not a stopword
    if low in NOUN_LEX:
        return "NOUN"
    return "NOUN"

# 4) Simple lemmatizer (rule-based; not stemming)
def lemmatize(tok, pos):
    low = tok.lower()

    # Proper nouns: keep original casing
    if pos == "PROPN":
        return tok

    # Verb lemmatization
    if pos == "VERB":
        # irregulars/known dictionary
        if low in {"was","were","been","being"}: return "be"
        if low in {"has","have","had"}: return "have"
        if low in {"does","did","done","doing"}: return "do"
        if low in {"goes","went","gone","going"}: return "go"
        if low in {"says","said","saying"}: return "say"
        if low in {"reads","reading","read"}: return "read"

        # suffix rules (very small, conservative)
        if low.endswith("ing") and len(low) > 4:
            base = low[:-3]
            # handle doubling (e.g., "running" -> "run") conservatively
            if len(base) >= 3 and base[-1] == base[-2]:
                base = base[:-1]
            return base
        if low.endswith("ed") and len(low) > 3:
            base = low[:-2]
            if base.endswith("i"):  # e.g., "tried" -> "try"
                base = base[:-1] + "y"
            return base
        if low.endswith("ies") and len(low) > 4:
            return low[:-3] + "y"     # "tries" -> "try"
        if low.endswith("es") and len(low) > 3:
            return low[:-2]           # "watches" -> "watch"
        if low.endswith("s") and not low.endswith("ss") and len(low) > 3:
            return low[:-1]           # "enjoys" -> "enjoy", "loves" -> "love"
        return low

    # Noun lemmatization (plural -> singular)
    if pos == "NOUN":
        if low.endswith("ies") and len(low) > 4:
            return low[:-3] + "y"     # "cities" -> "city"
        if low.endswith("s") and not low.endswith("ss") and len(low) > 3:
            return low[:-1]           # "books" -> "book"
        return low

    # Default
    return low

# ---- Run pipeline ----
tokens = tokenize(text)

# remove stopwords (case-insensitive) and non-alpha
tokens = [t for t in tokens if t.isalpha() and t.lower() not in STOPWORDS]

# POS tag (heuristic) and filter to NOUN/VERB/PROPN
tagged = [(t, guess_pos(t)) for t in tokens]
kept = [(t, p) for t, p in tagged if p in {"NOUN","VERB","PROPN"}]

# Lemmatize
final_tokens = [lemmatize(t, p) for t, p in kept]

print("Tokens:", tokens)
print("POS-kept:", kept)
print("Final tokens:", final_tokens)
print("As text:", " ".join(final_tokens))